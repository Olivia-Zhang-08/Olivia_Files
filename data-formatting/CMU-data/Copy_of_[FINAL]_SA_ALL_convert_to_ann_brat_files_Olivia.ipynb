{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of [FINAL] SA-ALL: convert to .ann/brat files_Olivia.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgSb2QU6FoaN",
        "colab_type": "code",
        "outputId": "2294af28-1959-4681-894e-820ac23ca325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# CONNECT TO GOOGLE DRIVE\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZuB42VNFxyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TRIM THE DATA TO THE RELEVANT COLUMNS\n",
        "\n",
        "# import pandas and numpy libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import trunc\n",
        "\n",
        "# read in the convo transcription file as a dataframe\n",
        "# this file should have simplified roles and cleaned text\n",
        "df = pd.read_csv('/content/gdrive/My Drive/Conversation Data Vis/Brown Annotated Convo Data/For LightSide/Cardiology1Clean_v2.csv')\n",
        "\n",
        "# define main file path\n",
        "annPath = '/content/gdrive/My Drive/Conversation Data Vis/Brown Annotated Convo Data/OLIVIA/formattingdata/ann_convert/SA-ALL/'\n",
        "\n",
        "# trim the data to only include relevant columns\n",
        "text = df[['ID', 'CleanText']]\n",
        "df = df[['ID','CleanText','Speech act','Induction','Topic code']]\n",
        "\n",
        "# save the trimmed data as a csv file in the drive for future reference\n",
        "df.to_csv(annPath + 'trimmedDF.csv', index = False)\n",
        "text.to_csv(annPath + 'justText.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sAOB-cTxPrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CREATE DICTIONARY OF ALL SPEECH ACT CODES\n",
        "SA_dict = {1:'general-q', 1.0:'general-q', 1.1:'Representative-q', 1.11:'open-q-Rep', 1.12:'closed-q-Rep', 1.121:'confirmation-q-Rep', 1.122:'clarification-q-Rep', 1.2:'Expressive-q', 1.21:'open-q-Exp', 1.22:'closed-q-Exp', 1.221:'confirmation-q-Exp', 1.222:'clarification-q-Exp', 1.23:'paraphrasing-reflection', 1.3:'affirm-knowledge', 1.31:'nonspecific-affirm', 1.32:'teachback-affirm', 2:'INFO-external-state', 2.1:'factual-info', 2.11:'self-report-behavior', 2.111:'past-ongoing-behavior', 2.12:'deduction', 2.121:'anticipate-future-action', 2.13:'confirm-statement', 2.131:'repetition-confirmation', 2.132:'finish-sentence-for', 2.14:'invoke-shared-info', 2.141:'accurate-teachback', 2.142:'inaccurate-teachback', 2.2:'comprehension-statement', 2.21:'past-comprehension', 2.3:'value-norm', 2.31:'past-value-norm', 2.4:'preferences-opinions', 2.401: 'past-preferences-opinions', 2.41:'compliment-praise', 2.42:'complain-criticize', 2.43:'positive-self-eval', 2.431:'past-positive-self-eval', 2.44:'negative-self-eval', 2.441:'past-negative-self-eval', 2.45:'concern-eval', 2.5:'motivational-ambivalence', 2.51:'past-motivational-ambivalence', 2.6:'desire-goal', 2.61:'past-desire-goal', 2.7:'intention', 2.71:'past-intention', 2.8:'self-efficacy', 2.801:'past-self-efficacy', 2.81:'negative-self-efficacy', 2.811:'past-negative-self-efficacy', 2.9:'affect-emotion', 2.91:'laughter', 2.92:'positive-affect', 2.921:'past-positive-affect', 2.93:'negative-affect', 2.931:'past-negative-affect', 2.94:'physical-pain', 2.941:'past-physical-pain', 2.95:'surprise-awe', 2.951:'past-surprise-awe', 2.96:'apathy', 2.961:'past-apathy', 2.97:'concern-anxiety', 2.971:'past-concern-anxiety', 2.98:'equanimity', 2.981:'past-equanimity', 3:'convo-management', 3.1:'facilitative-utt', 3.2:'transitional-buytime', 3.3:'introduce-topic', 3.4:'close-topic', 3.5:'introduce-preAgenda-topic', 3.6:'acknowledgement-utt', 3.7:'qualifying-utt', 4:'emotional-engagement', 4.1:'validating-engagement', 4.11:'overt-empathy', 4.12:'complex-reflection', 11:'continued-complex-reflection', 4.2:'negating-engagement', 4.21:'direct-negation', 5:'Directive', 5.1:'recommend', 5.2:'request', 5.3:'directive-aim', 5.4:'direct-command', 5.5:'convince', 5.6:'give-permission', 5.61:'refuse-permission', 5.7:'approve-of', 5.71:'disapprove-of', 6:'Commissive', 6.1:'ask-permission', 6.11:'current-intention', 6.2:'commit-to-action', 6.21:'agree-to-do', 6.22:'refuse-to-do', 6.3:'offer-to-do', 7:'Humor', 8:'Social-Ritual', 81:'MISSING-DATA', 82:'INCOMPLETE-UTT', 82.1:'INTERRUPTED', 83:'CODER-IDK', 99:'OOU'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmWsG6dg5ZKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SPLIT THE TEXT BY PATIENT\n",
        "\n",
        "# remember that the df called \"text\" is the df with just the patient ID and cleaned txt.\n",
        "\n",
        "# get row count for use in forloop\n",
        "rowCount = text.shape[0]\n",
        "\n",
        "# create new temporary empty dataframe with only text\n",
        "newText = pd.DataFrame(columns=['Text'])\n",
        "\n",
        "# initialize list for storing file names\n",
        "fileNames = []\n",
        "\n",
        "# evaluate the to_numpy array here, so as to not keep evaluating it in the for loop\n",
        "textval = text.to_numpy()\n",
        "\n",
        "# loop over each row in the data\n",
        "for i in range(0, rowCount):\n",
        "  \n",
        "  # need to append data to splitQ regardless of row or patient\n",
        "  newText = newText.append({'Text': textval[i,1]}, ignore_index = True)\n",
        "  \n",
        "  # code for splitting by patient\n",
        "  if (i == rowCount - 1 or textval[i,0] != textval[i+1,0]):\n",
        "    \n",
        "    # assign filename\n",
        "    filename = textval[i,0][0:9] + textval[i,0][10] + textval[i,0][12]\n",
        "    \n",
        "    # export new text\n",
        "    newText.to_csv(annPath + 'justText/' + filename + '.txt', index = False, header = False)\n",
        "    \n",
        "    # clear the buffer dataframe\n",
        "    newText = pd.DataFrame(columns=['Text'])\n",
        "    \n",
        "    # store filenames\n",
        "    fileNames.append(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lpCS7q2sbrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SPLIT ALL COLUMNS BY PATIENT\n",
        "\n",
        "# remember that the df called 'df' is the df with just the patient ID and cleaned txt.\n",
        "\n",
        "# get row count for use in forloop\n",
        "rowCount2 = df.shape[0]\n",
        "\n",
        "# create new temporary empty dataframe with only text\n",
        "newData = pd.DataFrame(columns=['ID','Text','Speech act','Induction','Topic code'])\n",
        "\n",
        "# initialize list for storing file names\n",
        "all_fileNames = []\n",
        "\n",
        "# evaluate the to_numpy array here, so as to not keep evaluating it in the for loop\n",
        "dfval = df.to_numpy()\n",
        "\n",
        "# loop over each row in the data\n",
        "for i in range(0, rowCount2):\n",
        "  \n",
        "  # need to append data to splitQ regardless of row or patient\n",
        "  newData = newData.append({'ID': dfval[i,0], 'Text': dfval[i,1], 'Speech act': dfval[i,2], 'Induction': dfval[i,3], 'Topic code': dfval[i,4]}, ignore_index = True)\n",
        "  \n",
        "  # code for splitting by patient\n",
        "  if (i == rowCount2 - 1 or dfval[i,0] != dfval[i+1,0]):\n",
        "    \n",
        "    # assign filename\n",
        "    filename = dfval[i,0][0:9] + dfval[i,0][10] + textval[i,0][12]\n",
        "    \n",
        "    # export new text\n",
        "    newData.to_csv(annPath + 'allData_split/' + filename + '.csv', index = False)\n",
        "    \n",
        "    # clear the buffer dataframe\n",
        "    newData = pd.DataFrame(columns=['ID','Text','Speech act','Induction','Topic code'])\n",
        "    \n",
        "    # store filenames\n",
        "    all_fileNames.append(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWlIL4S3lij8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CREATE .ANN FILES\n",
        "\n",
        "# initialize line index\n",
        "lineIndex = 0\n",
        "\n",
        "# initialize of brat entities (names)\n",
        "SA_ents = []\n",
        "\n",
        "SA_ents_txt = open(annPath + 'entities.txt', 'w+')\n",
        "\n",
        "# iterate through each file and open relevant files\n",
        "for file in fileNames:\n",
        "  f = open(annPath + 'justText/' + file + '.txt', 'r')\n",
        "  noQuotes = open(annPath +'noQuotes/nq' + file + '.txt', 'w+')\n",
        "  annForm = open(annPath + 'ann_form/ann' + file + '.ann', 'w+')\n",
        "  \n",
        "  bratTxt = open(annPath + 'brat_upload/brat' + file + '.txt', 'w+')\n",
        "  bratForm = open(annPath + 'brat_upload/brat' + file + '.ann', 'w+')\n",
        "  \n",
        "  SA_included = pd.read_csv(annPath + 'allData_split/' + file + '.csv')\n",
        "  SAval = SA_included.to_numpy()\n",
        "  \n",
        "  # initialize beginning variable and T_num (T# in .ann file)\n",
        "  beg = 0\n",
        "  T_num = 1\n",
        "  \n",
        "  # iterate through each line in the file\n",
        "  for line in f: \n",
        "\n",
        "    SA_code = trunc(SAval[lineIndex, 2]*1000)/1000\n",
        "    SA_type = SA_dict[SA_code]\n",
        "    SA_ent_name = (SA_type + '_' + str(SA_code) + '_').replace('.', '_')\n",
        "    \n",
        "    # get rid of quotation marks (which were automatically inputted due to .csv format) and save new files in the drive\n",
        "    nqline = line.replace('\"','')\n",
        "    noQuotes.write(nqline)\n",
        "    bratTxt.write(nqline)\n",
        "\n",
        "    # define end variable\n",
        "    end = beg + len(nqline) - 1\n",
        "    \n",
        "    # write in .ann format\n",
        "    annForm.write('T{}\\t{} {} {}\\t{}\\n'.format(T_num, SA_ent_name, beg, end, nqline))\n",
        "    bratForm.write('T{}\\t{} {} {}\\t{}\\n'.format(T_num, SA_ent_name, beg, end, nqline))\n",
        "    \n",
        "    T_num += 1\n",
        "    \n",
        "    if SA_ent_name not in SA_ents:\n",
        "      SA_ents.append(SA_ent_name)\n",
        "      SA_ents_txt.write(SA_ent_name + '\\n')\n",
        "    \n",
        "#     print(file,SA_code,SA_dict[SA_code])\n",
        "\n",
        "    # update beg and lineIndex\n",
        "    beg = end + 1\n",
        "    lineIndex += 1\n",
        "  \n",
        "  # reset lineIndex for next file\n",
        "  lineIndex = 0\n",
        "      \n",
        "  # close docs\n",
        "  f.close()\n",
        "  noQuotes.close()\n",
        "  annForm.close()\n",
        "  bratTxt.close()\n",
        "  bratForm.close()\n",
        "\n",
        "SA_ents_txt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7U_w_4IMe4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}